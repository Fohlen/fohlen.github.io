@inproceedings{jiang2023low,
  title={“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors},
  author={Jiang, Zhiying and Yang, Matthew and Tsirlin, Mikhail and Tang, Raphael and Dai, Yiqin and Lin, Jimmy},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={6810--6828},
  year={2023}
}

@article{teahan2003using,
  title={Using compression-based language models for text categorization},
  author={Teahan, William J and Harper, David J},
  journal={Language modeling for information retrieval},
  pages={141--165},
  year={2003},
  publisher={Springer}
}

@article{frank2000text,
  title={Text categorization using compression models},
  author={Frank, Eibe and Chui, Chang and Witten, Ian H},
  year={2000},
  publisher={University of Waikato, Department of Computer Science}
}

@article{piantadosi2014zipf,
  title={Zipf’s word frequency law in natural language: A critical review and future directions},
  author={Piantadosi, Steven T},
  journal={Psychonomic bulletin \& review},
  volume={21},
  pages={1112--1130},
  year={2014},
  publisher={Springer}
}

@article{681318,
  author={Bennett, C.H. and Gacs, P. and Ming Li and Vitanyi, P.M.B. and Zurek, W.H.},
  journal={IEEE Transactions on Information Theory}, 
  title={Information distance}, 
  year={1998},
  volume={44},
  number={4},
  pages={1407-1423},
  doi={10.1109/18.681318}
}

@article{levy-etal-2015-improving,
    title = "Improving Distributional Similarity with Lessons Learned from Word Embeddings",
    author = "Levy, Omer  and
      Goldberg, Yoav  and
      Dagan, Ido",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q15-1016",
    doi = "10.1162/tacl_a_00134",
    pages = "211--225",
    abstract = "Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.",
}
