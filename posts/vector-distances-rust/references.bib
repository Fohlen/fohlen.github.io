@article{steck2024cosine,
  title={Is Cosine-Similarity of Embeddings Really About Similarity?},
  author={Steck, Harald and Ekanadham, Chaitanya and Kallus, Nathan},
  journal={arXiv preprint arXiv:2403.05440},
  year={2024}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{bojanowski2016enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.04606},
  year={2016}
}

@misc{Wenkel_2024, 
  title={Cosine distance implementations in Python}, 
  url={https://simonwenkel.com/notes/ai/metrics/cosine_distance.html}, 
  journal={simonwenkel.com}, 
  author={Wenkel, Simon}, 
  year={2024}
}

@book{Jurafsky2008-yj,
  title     = "Speech and language processing",
  author    = "Jurafsky, Dan and Martin, James H",
  publisher = "Pearson",
  series    = "Prentice Hall Series in Artificial Intelligence",
  edition   =  2,
  month     =  apr,
  year      =  2008,
  address   = "Upper Saddle River, NJ",
  language  = "en",
  chapter   = "6.10"
}

@InProceedings{10.1007/978-3-642-36973-5_36,
author="Dang, Van
and Bendersky, Michael
and Croft, W. Bruce",
editor="Serdyukov, Pavel
and Braslavski, Pavel
and Kuznetsov, Sergei O.
and Kamps, Jaap
and R{\"u}ger, Stefan
and Agichtein, Eugene
and Segalovich, Ilya
and Yilmaz, Emine",
title="Two-Stage Learning to Rank for Information Retrieval",
booktitle="Advances in Information Retrieval",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="423--434",
abstract="Current learning to rank approaches commonly focus on learning the best possible ranking function given a small fixed set of documents. This document set is often retrieved from the collection using a simple unsupervised bag-of-words method, e.g. BM25. This can potentially lead to learning a sub-optimal ranking, since many relevant documents may be excluded from the initially retrieved set. In this paper we propose a novel two-stage learning framework to address this problem. We first learn a ranking function over the entire retrieval collection using a limited set of textual features including weighted phrases, proximities and expansion terms. This function is then used to retrieve the best possible subset of documents over which the final model is trained using a larger set of query- and document-dependent features. Empirical evaluation using two web collections unequivocally demonstrates that our proposed two-stage framework, being able to learn its model from more relevant documents, outperforms current learning to rank approaches.",
isbn="978-3-642-36973-5"
}


